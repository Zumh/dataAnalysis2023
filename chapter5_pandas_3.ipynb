{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catenating Datasets in Pandas\n",
    "\n",
    "#### Vertical Catenation (axis=0)\n",
    "```python\n",
    "# Helper function to create DataFrames\n",
    "def makedf(cols, ind):\n",
    "    data = {c: [str(c) + str(i) for i in ind] for c in cols}\n",
    "    return pd.DataFrame(data, ind)\n",
    "\n",
    "# Example DataFrames\n",
    "a = makedf(\"AB\", [0,1])\n",
    "b = makedf(\"AB\", [2,3])\n",
    "c = makedf(\"CD\", [0,1])\n",
    "d = makedf(\"BC\", [2,3])\n",
    "\n",
    "# Concatenating vertically\n",
    "result1 = pd.concat([a, b])\n",
    "```\n",
    "Result:\n",
    "```\n",
    "    A   B\n",
    "0  A0  B0\n",
    "1  A1  B1\n",
    "2  A2  B2\n",
    "3  A3  B3\n",
    "```\n",
    "\n",
    "#### Duplicate Indices\n",
    "```python\n",
    "# Concatenating with duplicate indices\n",
    "result2 = pd.concat([a, a])\n",
    "# Handling duplicate indices\n",
    "result3 = pd.concat([a, a], ignore_index=True)\n",
    "result4 = pd.concat([a, a], keys=['first', 'second'])\n",
    "```\n",
    "Result (`result2` with duplicate indices):\n",
    "```\n",
    "    A   B\n",
    "0  A0  B0\n",
    "1  A1  B1\n",
    "0  A0  B0\n",
    "1  A1  B1\n",
    "```\n",
    "Result (`result3` with renumbered indices):\n",
    "```\n",
    "    A   B\n",
    "0  A0  B0\n",
    "1  A1  B1\n",
    "2  A0  B0\n",
    "3  A1  B1\n",
    "```\n",
    "Result (`result4` with hierarchical indexing):\n",
    "```\n",
    "         A   B\n",
    "first  0  A0  B0\n",
    "       1  A1  B1\n",
    "second 0  A0  B0\n",
    "       1  A1  B1\n",
    "```\n",
    "\n",
    "#### Horizontal Catenation (axis=1)\n",
    "```python\n",
    "# Concatenating horizontally\n",
    "result5 = pd.concat([a, c], axis=1)\n",
    "```\n",
    "Result:\n",
    "```\n",
    "    A   B   C   D\n",
    "0  A0  B0  C0  D0\n",
    "1  A1  B1  C1  D1\n",
    "```\n",
    "\n",
    "#### Handling Different Columns and Indices\n",
    "```python\n",
    "# Outer join (union of columns)\n",
    "result6 = pd.concat([a, d], sort=False)\n",
    "# Inner join (intersection of columns)\n",
    "result7 = pd.concat([a, d], join=\"inner\")\n",
    "```\n",
    "Result (`result6` with outer join and NaNs):\n",
    "```\n",
    "     A    B    C\n",
    "0   A0   B0  NaN\n",
    "1   A1   B1  NaN\n",
    "2  NaN   B2   C2\n",
    "3  NaN   B3   C3\n",
    "```\n",
    "Result (`result7` with inner join):\n",
    "```\n",
    "    B\n",
    "0  B0\n",
    "1  B1\n",
    "2  B2\n",
    "3  B3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (37128, 25)\n",
      "Column names:\n",
      " Index(['Weekday', 'Day', 'Month', 'Year', 'Hour', 'Auroransilta',\n",
      "       'Eteläesplanadi', 'Huopalahti (asema)', 'Kaisaniemi/Eläintarhanlahti',\n",
      "       'Kaivokatu', 'Kulosaaren silta et.', 'Kulosaaren silta po. ',\n",
      "       'Kuusisaarentie', 'Käpylä, Pohjoisbaana',\n",
      "       'Lauttasaaren silta eteläpuoli', 'Merikannontie',\n",
      "       'Munkkiniemen silta eteläpuoli', 'Munkkiniemi silta pohjoispuoli',\n",
      "       'Heperian puisto/Ooppera', 'Pitkäsilta itäpuoli',\n",
      "       'Pitkäsilta länsipuoli', 'Lauttasaaren silta pohjoispuoli',\n",
      "       'Ratapihantie', 'Viikintie', 'Baana'],\n",
      "      dtype='object')\n",
      "  Weekday  Day  Month  Year  Hour  Auroransilta  Eteläesplanadi  \\\n",
      "0     Wed    1      1  2014     0           NaN             7.0   \n",
      "1     Wed    1      1  2014     1           NaN             5.0   \n",
      "2     Wed    1      1  2014     2           NaN             2.0   \n",
      "3     Wed    1      1  2014     3           NaN             5.0   \n",
      "4     Wed    1      1  2014     4           NaN             1.0   \n",
      "\n",
      "   Huopalahti (asema)  Kaisaniemi/Eläintarhanlahti  Kaivokatu  ...  \\\n",
      "0                 NaN                          1.0        NaN  ...   \n",
      "1                 NaN                          3.0        NaN  ...   \n",
      "2                 NaN                          3.0        NaN  ...   \n",
      "3                 NaN                          2.0        NaN  ...   \n",
      "4                 NaN                          4.0        NaN  ...   \n",
      "\n",
      "   Merikannontie  Munkkiniemen silta eteläpuoli  \\\n",
      "0            NaN                            2.0   \n",
      "1            NaN                            6.0   \n",
      "2            NaN                            1.0   \n",
      "3            NaN                            0.0   \n",
      "4            NaN                            1.0   \n",
      "\n",
      "   Munkkiniemi silta pohjoispuoli  Heperian puisto/Ooppera  \\\n",
      "0                             5.0                      3.0   \n",
      "1                             5.0                      1.0   \n",
      "2                             1.0                      1.0   \n",
      "3                             2.0                      0.0   \n",
      "4                             1.0                      1.0   \n",
      "\n",
      "   Pitkäsilta itäpuoli  Pitkäsilta länsipuoli  \\\n",
      "0                  NaN                   11.0   \n",
      "1                  NaN                    8.0   \n",
      "2                  NaN                   14.0   \n",
      "3                  NaN                    7.0   \n",
      "4                  NaN                    9.0   \n",
      "\n",
      "   Lauttasaaren silta pohjoispuoli  Ratapihantie  Viikintie  Baana  \n",
      "0                              NaN           NaN        NaN    8.0  \n",
      "1                              NaN           NaN        NaN    4.0  \n",
      "2                              NaN           NaN        NaN   11.0  \n",
      "3                              NaN           NaN        NaN    3.0  \n",
      "4                              NaN           NaN        NaN    4.0  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 5.1 (split date continues)\n",
    "Write function split_date_continues that does\n",
    "\n",
    "read the bicycle data set\n",
    "clean the data set of columns/rows that contain only missing values\n",
    "\n",
    "drops the Päivämäärä column and replaces it with its splitted components as before\n",
    "\n",
    "Use the concat function to do this.\n",
    "\n",
    "The function should return a DataFrame with 25 columns (first five related to the date and then the rest 20 concerning the measument location.\n",
    "\n",
    "Hint: You may use your solution or the model solution from exercise 16 of the previous set as a starting point.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "days = dict(zip(\"ma ti ke to pe la su\".split(), \"Mon Tue Wed Thu Fri Sat Sun\".split()))\n",
    "months = dict(zip(\"tammi helmi maalis huhti touko kesä heinä elo syys loka marras joulu\".split(), range(1,13)))\n",
    "def split_date():\n",
    "    df = pd.read_csv(\"src/Helsingin_pyorailijamaarat.csv\", sep=\";\")\n",
    "    df = df.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "    d = df[\"Päivämäärä\"].str.split(expand=True)\n",
    "    d.columns = [\"Weekday\", \"Day\", \"Month\", \"Year\", \"Hour\"]\n",
    " \n",
    "    hourmin = d[\"Hour\"].str.split(\":\", expand=True)\n",
    "    d[\"Hour\"] = hourmin.iloc[:,0]\n",
    " \n",
    "    d[\"Weekday\"] = d[\"Weekday\"].map(days)\n",
    "    d[\"Month\"] = d[\"Month\"].map(months)\n",
    "    \n",
    "    d = d.astype({\"Weekday\": object, \"Day\": int, \"Month\": int, \"Year\": int, \"Hour\": int})\n",
    "    return d\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "days = dict(zip(\"ma ti ke to pe la su\".split(), \"Mon Tue Wed Thu Fri Sat Sun\".split()))\n",
    "months = dict(zip(\"tammi helmi maalis huhti touko kesä heinä elo syys loka marras joulu\".split(), range(1,13)))\n",
    "def split_date(df):\n",
    "   \n",
    "    \n",
    "    d = df[\"Päivämäärä\"].str.split(expand=True)\n",
    "    d.columns = [\"Weekday\", \"Day\", \"Month\", \"Year\", \"Hour\"]\n",
    " \n",
    "    hourmin = d[\"Hour\"].str.split(\":\", expand=True)\n",
    "    d[\"Hour\"] = hourmin.iloc[:,0]\n",
    " \n",
    "    d[\"Weekday\"] = d[\"Weekday\"].map(days)\n",
    "    d[\"Month\"] = d[\"Month\"].map(months)\n",
    "\n",
    "    \n",
    "    d = d.astype({\"Weekday\": object, \"Day\": int, \"Month\": int, \"Year\": int, \"Hour\": int})\n",
    "    return d\n",
    "\n",
    "\n",
    "def split_date_continues():\n",
    "    df = pd.read_csv(\"part05-e01_split_date_continues/src/Helsingin_pyorailijamaarat.csv\", sep=';')\n",
    "\n",
    "    # cleaning all the missing values\n",
    "    df = df.dropna(axis = 0, how='all').dropna(axis = 1, how='all')\n",
    "\n",
    "    # first 5 column for date format\n",
    "    date_format = split_date(df)\n",
    "    \n",
    "    # drop Päivämäärä\n",
    "    df = df.drop('Päivämäärä', axis=1) \n",
    "   \n",
    "    \n",
    "    # rest 20 concerning the measurment location\n",
    "    df = pd.concat([date_format, df], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    " \n",
    "\n",
    "def main():\n",
    "    df = split_date_continues()\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Column names:\\n\", df.columns)\n",
    "    print(df.head())\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging DataFrames in Pandas\n",
    "\n",
    "#### One-to-One Merge\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Original DataFrames\n",
    "df = pd.DataFrame([[1000, \"Jack\", 21], [1500, \"John\", 29]], columns=[\"Wage\", \"Name\", \"Age\"])\n",
    "df2 = pd.DataFrame({\"Name\" : [\"John\", \"Jack\"], \"Occupation\": [\"Plumber\", \"Carpenter\"]})\n",
    "\n",
    "# One-to-One Merge\n",
    "result_df = pd.merge(df, df2)\n",
    "```\n",
    "\n",
    "#### Inner Join with Missing Keys\n",
    "```python\n",
    "# Adding a person without wage and age\n",
    "df3 = pd.concat([df2, pd.DataFrame({ \"Name\" : [\"James\"], \"Occupation\":[\"Painter\"]})], ignore_index=True)\n",
    "\n",
    "# Inner Join (default behavior)\n",
    "result_inner_join = pd.merge(df, df3)\n",
    "```\n",
    "\n",
    "#### Outer Join\n",
    "```python\n",
    "# Outer Join\n",
    "result_outer_join = pd.merge(df, df3, how=\"outer\")\n",
    "```\n",
    "\n",
    "#### Many-to-One and Many-to-Many Relationships\n",
    "```python\n",
    "# Books and Collections DataFrames\n",
    "books = pd.DataFrame({\"Title\" : [\"War and Peace\", \"Good Omens\", \"Good Omens\"] , \n",
    "                      \"Author\" : [\"Tolstoi\", \"Terry Pratchett\", \"Neil Gaiman\"]})\n",
    "collections = pd.DataFrame([[\"Oodi\", \"War and Peace\"],\n",
    "                           [\"Oodi\", \"Good Omens\"],\n",
    "                           [\"Pasila\", \"Good Omens\"],\n",
    "                           [\"Kallio\", \"War and Peace\"]], columns=[\"Library\", \"Title\"])\n",
    "\n",
    "# Many-to-Many Merge\n",
    "libraries_with_books_by = pd.merge(books, collections)\n",
    "```\n",
    "\n",
    "### Summary:\n",
    "- One-to-One Merge: Merging based on a common field.\n",
    "- Inner Join: Default behavior, keeps only matching keys.\n",
    "- Outer Join: Includes all keys from both DataFrames.\n",
    "- Many-to-One and Many-to-Many Merges: Handling relationships where keys may repeat in one or both DataFrames.\n",
    "\n",
    "These concepts are essential for combining and analyzing data from different sources in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8760, 28)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 5.2 (cycling weather)\n",
    "Merge the processed cycling data set (from the previous exercise) and \n",
    "weather data set along the columns year, month, and day. \n",
    "\n",
    "Note that the names of these columns might be different in the two tables: \n",
    "use the left_on and right_on parameters. Then drop useless columns 'm', 'd', 'Time', and 'Time zone'.\n",
    "\n",
    "Write function cycling_weather that reads the data sets and returns the resulting DataFrame.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "days = dict(zip(\"ma ti ke to pe la su\".split(), \"Mon Tue Wed Thu Fri Sat Sun\".split()))\n",
    "months = dict(zip(\"tammi helmi maalis huhti touko kesä heinä elo syys loka marras joulu\".split(), range(1,13)))\n",
    "def split_date(df):\n",
    "   \n",
    "    \n",
    "    d = df[\"Päivämäärä\"].str.split(expand=True)\n",
    "    d.columns = [\"Weekday\", \"Day\", \"Month\", \"Year\", \"Hour\"]\n",
    " \n",
    "    hourmin = d[\"Hour\"].str.split(\":\", expand=True)\n",
    "    d[\"Hour\"] = hourmin.iloc[:,0]\n",
    " \n",
    "    d[\"Weekday\"] = d[\"Weekday\"].map(days)\n",
    "    d[\"Month\"] = d[\"Month\"].map(months)\n",
    "\n",
    "    \n",
    "    d = d.astype({\"Weekday\": object, \"Day\": int, \"Month\": int, \"Year\": int, \"Hour\": int})\n",
    "    return d\n",
    "\n",
    "\n",
    "def split_date_continues():\n",
    "    df = pd.read_csv(\"part05-e02_cycling_weather/src/Helsingin_pyorailijamaarat.csv\", sep=';')\n",
    "\n",
    "    # cleaning all the missing values\n",
    "    df = df.dropna(axis = 0, how='all').dropna(axis = 1, how='all')\n",
    "\n",
    "    # first 5 column for date format\n",
    "    date_format = split_date(df)\n",
    "    \n",
    "    # drop Päivämäärä\n",
    "    df = df.drop('Päivämäärä', axis=1) \n",
    "   \n",
    "    \n",
    "    # rest 20 concerning the measurment location\n",
    "    df = pd.concat([date_format, df], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def cycling_weather():\n",
    "    \n",
    "    cycling_data_set = split_date_continues()\n",
    "    weather_data_set = pd.read_csv(\"part05-e02_cycling_weather/src/kumpula-weather-2017.csv\", sep=',')\n",
    " \n",
    "  \n",
    "    \n",
    "    merge_data = pd.merge(cycling_data_set, weather_data_set, left_on=['Year','Month','Day'], right_on=['Year','m','d'])\n",
    "    drop_columns = ['m','d','Time','Time zone']\n",
    "   \n",
    "    merge_data = merge_data.drop(columns=drop_columns)\n",
    "\n",
    "    return(merge_data)\n",
    "\n",
    "\"\"\"def cycling_weather():\n",
    "    wh = pd.read_csv(\"src/kumpula-weather-2017.csv\")\n",
    "    bike = split_date_continues()\n",
    "    result = pd.merge(wh, bike, left_on=[\"Year\", \"m\", \"d\"], right_on=[\"Year\", \"Month\", \"Day\"])\n",
    " \n",
    "    return result.drop(['m', 'd', 'Time', 'Time zone'], axis=1)\"\"\"\n",
    "\n",
    "def main():\n",
    "    cycling_weather()\n",
    "    return\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                    The Beatles\n",
      "1            The Dave Clark Five\n",
      "2       Freddie And The Dreamers\n",
      "3                    The Shadows\n",
      "4             The Rolling Stones\n",
      "5       Gerry And The Pacemakers\n",
      "6                    The Hollies\n",
      "7    Johnny Kidd And The Pirates\n",
      "Name: Band, dtype: object\n",
      "0                              The Beatles\n",
      "1                      The Dave Clark Five\n",
      "2                              The Beatles\n",
      "3                 Freddie And The Dreamers\n",
      "4                              Gene Pitney\n",
      "5                        Dusty Springfield\n",
      "6                          The Singing Nun\n",
      "7                     Los Indios Tabajaras\n",
      "8                              Kathy Kirby\n",
      "9                            Cliff Richard\n",
      "10                           Big Dee Irwin\n",
      "11                             The Shadows\n",
      "12                     Swinging Blue Jeans\n",
      "13                           Elvis Presley\n",
      "14                      The Rolling Stones\n",
      "15                Gerry And The Pacemakers\n",
      "16                             The Hollies\n",
      "17                          Chris Sandford\n",
      "18             Bern Elliott And The Fenmen\n",
      "19                              Adam Faith\n",
      "20          Billy J Kramer And The Dakotas\n",
      "21                           Harry Secombe\n",
      "22                             Roy Orbison\n",
      "23                              Dora Bryan\n",
      "24                          Shirley Bassey\n",
      "25    Wilfred Brambell And Harry H Corbett\n",
      "26                                   Heinz\n",
      "27                             Buddy Holly\n",
      "28                              Matt Monro\n",
      "29                             Mark Wynter\n",
      "30             Johnny Kidd And The Pirates\n",
      "31                                Fourmost\n",
      "32           Brian Poole And The Tremeloes\n",
      "33            Nino Tempo And April Stevens\n",
      "34                    Peter, Paul And Mary\n",
      "35                             Chuck Berry\n",
      "36                              Billy Fury\n",
      "37                           The Searchers\n",
      "38            Chad Stuart And Jeremy Clyde\n",
      "39                         Richard Anthony\n",
      "Name: Artist, dtype: object\n",
      "(9, 13)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Exercise 5.3 (top bands)\n",
    "Merge the DataFrames UK top40 and the bands DataFrame that are stored in the src folder. \n",
    "Do all this in the parameterless function top_bands, which should return the merged DataFrame. \n",
    "Use the left_on and right_on parameters to merge. Test your function from the main function.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def top_bands():\n",
    "    # top40UK\n",
    "    top40UK = pd.read_csv(\"part05-e03_top_bands/src/UK-top40-1964-1-2.tsv\", sep='\\t')\n",
    "    bands = pd.read_csv(\"part05-e03_top_bands/src/bands.tsv\", sep='\\t')\n",
    "    bands['Band'] = bands['Band'].str.title()\n",
    "    top40UK['Artist'] =  top40UK['Artist'].str.title()\n",
    "  \n",
    "    merge_data = pd.merge(top40UK, bands, left_on='Artist', right_on='Band')\n",
    "   \n",
    "    return merge_data\n",
    "\n",
    "def main():\n",
    "    top_bands()\n",
    "    return\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dstl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
